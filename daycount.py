import sysfrom csv import readerfrom pyspark import SparkContextsc = SparkContext()data = sc.textFile(sys.argv[1], 1).mapPartitions(lambda x: reader(x))result = data.map(lambda x: (x[11][:10], 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[0])output = result.map(lambda x: x[0] + ',' + str(x[1]))output.saveAsTextFile("daycount.out")'''module load python/gnu/3.6.5module load spark/2.4.0rm -rf daycount.outhfs -rm -R daycount.outspark-submit --conf \spark.pyspark.python=/share/apps/python/3.6.5/bin/python \daycount.py Historical_Driver_Application_Status.csvhfs -getmerge daycount.out daycount.outhfs -rm -R daycount.outhead daycount.outtail daycount.out'''